{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7c8e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d7b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf19fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# create a random environment\n",
    "env = gym.make('FrozenLake-v1', \n",
    "               desc=generate_random_map(size=5), \n",
    "               is_slippery=False, \n",
    "               render_mode='ansi', \n",
    "               max_episode_steps=1000)\n",
    "# help(env)\n",
    "print(env.spec.max_episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "748ea418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the environment\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name='8x8', is_slippery=False, render_mode='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac81e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mHFF\n",
      "FFHFF\n",
      "FFFFF\n",
      "FFFFF\n",
      "FFFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking out the environment\n",
    "env.reset()\n",
    "\n",
    "for step in range(15):\n",
    "    print(env.render())\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, trunc, info = env.step(action)\n",
    "    time.sleep(0.2)\n",
    "    clear_output(wait=True)\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c57999a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "EPOCHS = 20000 #episodes, how many times the agents plays the game until it hits done\n",
    "ALPHA = 0.8 # LEARNING RATE\n",
    "GAMMA = 0.95 # DISCOUNT RATE\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb1cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_q_table():\n",
    "    \n",
    "    action_size = env.action_space.n\n",
    "    state_size = env.observation_space.n\n",
    "\n",
    "    #rows: states, columns: actions\n",
    "    q_table = np.zeros([state_size, action_size])\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c7bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(epsilon, q_table, discrete_state):\n",
    "    \n",
    "    random_number = np.random.random()\n",
    "    \n",
    "    #EXPLOITATION (choose the action that maximizes Q)\n",
    "    if random_number > epsilon:\n",
    "        \n",
    "        state_row = q_table[discrete_state, :]\n",
    "        action = np.argmax(state_row)\n",
    "    \n",
    "    \n",
    "    #EXPLORATION (choose a random action)\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88550ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_next_q_value(old_q_value, reward, next_optimal_q_value):\n",
    "    \n",
    "    return old_q_value + ALPHA * (reward + GAMMA*next_optimal_q_value - old_q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ec9569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponential decay of epsilon\n",
    "def reduce_epsilon(epsilon, epoch):\n",
    "    \n",
    "    return min_epsilon + (max_epsilon-min_epsilon)*np.exp(-decay_rate*epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ccc03b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epsilon):\n",
    "\n",
    "    debug = False\n",
    "\n",
    "    rewards = []\n",
    "    log_interval = 1000\n",
    "\n",
    "    # reset q_table\n",
    "    q_table = reset_q_table()\n",
    "\n",
    "    for episode in range(EPOCHS):\n",
    "\n",
    "        if debug: print(f\"============= running episode: {episode} of {EPOCHS} =================\")\n",
    "\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            # action\n",
    "            action = epsilon_greedy_action_selection(epsilon, q_table, state)\n",
    "\n",
    "            # state, reward... env.stepp()\n",
    "            new_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "            # OLD == CURRENT Q VALUE\n",
    "            old_q_value = q_table[state, action]\n",
    "\n",
    "            # get next optimal Q value\n",
    "            next_optimal_q_value = np.max(q_table[new_state, :])\n",
    "\n",
    "            # compute the next Q value\n",
    "            next_q = compute_next_q_value(old_q_value, reward, next_optimal_q_value)\n",
    "\n",
    "            # update the table\n",
    "            q_table[state, action] = next_q\n",
    "\n",
    "            # track rewards\n",
    "            total_rewards += reward\n",
    "\n",
    "            # new state is now the state\n",
    "            state = new_state\n",
    "\n",
    "        if debug: print(q_table)\n",
    "\n",
    "        # agent finsihed a round of the game\n",
    "        episode += 1\n",
    "\n",
    "        epsilon = reduce_epsilon(epsilon, episode)\n",
    "\n",
    "        rewards.append(total_rewards)\n",
    "\n",
    "        if episode % log_interval == 0:\n",
    "            print(np.sum(rewards))\n",
    "\n",
    "    env.close()\n",
    "    return q_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf736a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706.0\n",
      "1693.0\n",
      "2691.0\n",
      "3691.0\n",
      "4691.0\n",
      "5691.0\n",
      "6691.0\n",
      "7691.0\n",
      "8691.0\n",
      "9690.0\n",
      "10690.0\n",
      "11690.0\n",
      "12690.0\n",
      "13690.0\n",
      "14690.0\n",
      "15690.0\n",
      "16690.0\n",
      "17690.0\n",
      "18690.0\n",
      "19690.0\n"
     ]
    }
   ],
   "source": [
    "# run the training\n",
    "q_table = train(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6126bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFHFF\n",
      "FFHFF\n",
      "FFFFF\n",
      "FFFFF\n",
      "FFF\u001b[41mF\u001b[0mG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets see how the agent is performing after training\n",
    "state = env.reset()[0]\n",
    "\n",
    "for steps in range(100):\n",
    "    print(env.render())\n",
    "    action = np.argmax(q_table[state, :])\n",
    "    state, reward, done, trunc, info = env.step(action)\n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12088390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
